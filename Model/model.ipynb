{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: librosa in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: torch in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pycparser in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kartik-dua/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing required Python packages for the project\n",
    "# This command works inside Jupyter notebooks specifically using the %pip magic\n",
    "\n",
    "%pip install numpy pandas librosa scikit-learn torch torchvision torchaudio tqdm matplotlib seaborn\n",
    "\n",
    "# ------------------------\n",
    "# Libraries included:\n",
    "# numpy        - for numerical computing and matrix operations\n",
    "# pandas       - for working with structured tabular data (DataFrames)\n",
    "# librosa      - for audio analysis and feature extraction like mel spectrograms\n",
    "# scikit-learn - for ML utilities (splits, metrics, normalization, etc.)\n",
    "# torch        - PyTorch for building and training neural networks\n",
    "# torchvision  - for computer vision utilities; may be unused if no vision features\n",
    "# torchaudio   - for PyTorch-native audio handling\n",
    "# tqdm         - for nice progress bars during long loops/training\n",
    "# matplotlib   - for creating visualizations\n",
    "# seaborn      - for prettier statistical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa       # for audio loading and spectrogram generation\n",
    "import numpy as np   # for array manipulation and padding\n",
    "import torch         # for tensor handling and model input formatting\n",
    "\n",
    "def extract_features(file_path, sample_rate=16000, duration=3, n_mels=128):\n",
    "    \"\"\"\n",
    "    Loads an audio file, ensures it has a fixed duration, extracts log-Mel spectrogram features,\n",
    "    and converts the result into a PyTorch tensor.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the audio file\n",
    "        sample_rate (int): Target sampling rate (default: 16kHz)\n",
    "        duration (int): Target duration in seconds (default: 3s)\n",
    "        n_mels (int): Number of Mel filterbanks (default: 128)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape [1, n_mels, TimeSteps]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and resample the audio to the desired sample rate\n",
    "        signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "\n",
    "        # Ensure the audio is exactly `duration` seconds long\n",
    "        required_len = sample_rate * duration\n",
    "        if len(signal) > required_len:\n",
    "            signal = signal[:required_len]  # Truncate\n",
    "        else:\n",
    "            signal = np.pad(signal, (0, required_len - len(signal)))  # Pad\n",
    "\n",
    "        # Compute Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=signal,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "\n",
    "        # Convert spectrogram to log scale (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "\n",
    "        # Convert to PyTorch tensor and add channel dimension: [1, n_mels, time]\n",
    "        return torch.tensor(log_mel_spec).unsqueeze(0).float()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        # Return a zero tensor with approximate shape if anything goes wrong\n",
    "        return torch.zeros((1, n_mels, int(sample_rate * duration / 512)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>unused</th>\n",
       "      <th>system_id</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4-727-124443-0055</td>\n",
       "      <td>-</td>\n",
       "      <td>baidu</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5-2012-139356-0032</td>\n",
       "      <td>-</td>\n",
       "      <td>baidu</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>103-534-127537-0043</td>\n",
       "      <td>-</td>\n",
       "      <td>baidu</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>111-5266-34501-0003</td>\n",
       "      <td>-</td>\n",
       "      <td>baidu</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>111-203-132069-0025</td>\n",
       "      <td>-</td>\n",
       "      <td>baidu</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker_id            file_name unused system_id    key\n",
       "0          4    4-727-124443-0055      -     baidu  spoof\n",
       "1          5   5-2012-139356-0032      -     baidu  spoof\n",
       "2        103  103-534-127537-0043      -     baidu  spoof\n",
       "3        111  111-5266-34501-0003      -     baidu  spoof\n",
       "4        111  111-203-132069-0025      -     baidu  spoof"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  # for structured data loading and manipulation\n",
    "\n",
    "def load_protocol(file_path):\n",
    "    \"\"\"\n",
    "    Loads a protocol file (e.g., en_train.txt) into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Full path to the protocol file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with labeled columns:\n",
    "                      ['speaker_id', 'file_name', 'unused', 'system_id', 'key']\n",
    "    \"\"\"\n",
    "    columns = ['speaker_id', 'file_name', 'unused', 'system_id', 'key']\n",
    "    return pd.read_csv(file_path, sep=' ', names=columns)\n",
    "\n",
    "# Example usage: loading English training protocol\n",
    "df_en_train = load_protocol(\"/home/kartik-dua/python_project/DECRO/inner/en_train.txt\")\n",
    "df_en_train.head()  # Displays the first 5 rows for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa  # for audio processing and spectrogram generation\n",
    "import numpy as np  # for numerical ops like padding\n",
    "\n",
    "def extract_log_mel(file_path, sr=16000, n_mels=64, duration=3.0):\n",
    "    \"\"\"\n",
    "    Extracts log-Mel spectrogram from an audio file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the audio file (.wav, .flac, etc.)\n",
    "        sr (int): Target sample rate for resampling (default: 16000 Hz)\n",
    "        n_mels (int): Number of Mel filter banks (default: 64)\n",
    "        duration (float): Desired duration in seconds (default: 3.0)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D array of shape [n_mels, time_frames] representing log-Mel spectrogram\n",
    "    \"\"\"\n",
    "    # Load audio file, resample, trim to specified duration\n",
    "    audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "\n",
    "    # Zero-pad if shorter than required length\n",
    "    if len(audio) < int(sr * duration):\n",
    "        pad_width = int(sr * duration) - len(audio)\n",
    "        audio = np.pad(audio, (0, pad_width))\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "\n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "\n",
    "    return log_mel_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class DECRODataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading and processing the DECRO audio dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing protocol metadata with 'file_name' and 'key' columns.\n",
    "        base_path (str): Path to the directory containing the corresponding .wav audio files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, base_path):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and processes one sample given an index.\n",
    "\n",
    "        Returns:\n",
    "            feature (Tensor): Log-Mel spectrogram tensor of shape (1, n_mels, time_frames)\n",
    "            label (int): Binary label (1 = spoof, 0 = real)\n",
    "        \"\"\"\n",
    "        # Access the row in the DataFrame\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(self.base_path, row['file_name'] + \".wav\")\n",
    "\n",
    "        # Assign binary label: 1 for spoof, 0 for real\n",
    "        label = 1 if row['key'] == 'spoof' else 0\n",
    "\n",
    "        # Extract log-Mel spectrogram features\n",
    "        feature = extract_log_mel(file_path)\n",
    "\n",
    "        # Convert to PyTorch tensor and add channel dimension\n",
    "        feature = torch.tensor(feature).unsqueeze(0).float()\n",
    "\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for file handling and data operations\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset  # Utility to create custom datasets for DataLoader\n",
    "\n",
    "# === Function to load the protocol metadata ===\n",
    "def load_protocol(file_path):\n",
    "    # Load the protocol file assuming 5 space-separated columns\n",
    "    columns = ['speaker_id', 'file_name', 'system_id', 'dash', 'label']\n",
    "    df = pd.read_csv(file_path, sep=\" \", names=columns)\n",
    "    # We only need filename and label for dataset use\n",
    "    return df[['file_name', 'label']]\n",
    "\n",
    "# === Function to extract log-Mel spectrogram features from an audio file ===\n",
    "def extract_features(file_path, sample_rate=16000, duration=3, n_mels=128):\n",
    "    try:\n",
    "        # Load audio at given sample rate\n",
    "        signal, sr = librosa.load(file_path, sr=sample_rate)\n",
    "\n",
    "        # Pad or truncate to get fixed-duration audio\n",
    "        required_length = sample_rate * duration\n",
    "        if len(signal) > required_length:\n",
    "            signal = signal[:required_length]\n",
    "        else:\n",
    "            signal = np.pad(signal, (0, required_length - len(signal)))\n",
    "\n",
    "        # Compute Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=n_mels)\n",
    "\n",
    "        # Convert power spectrogram to decibel scale (log-Mel)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "\n",
    "        # Convert to 3D tensor [1, n_mels, time] suitable for CNN input\n",
    "        return torch.tensor(log_mel_spec).unsqueeze(0).float()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return a zero tensor in case of error (e.g., corrupted file)\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return torch.zeros((1, n_mels, int(required_length//512)))  # fallback shape\n",
    "\n",
    "# === Custom PyTorch Dataset class for DECRO Audio Deepfake detection ===\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, protocol_file, audio_dir, sample_rate=16000, duration=3, n_mels=128):\n",
    "        # Load protocol DataFrame with file names and labels\n",
    "        self.df = load_protocol(protocol_file)\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "        # Audio preprocessing parameters\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        # Convert label strings to numeric: bonafide → 0, spoof → 1\n",
    "        self.label_map = {'bonafide': 0, 'spoof': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get one row of metadata\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Build full path to the .wav file\n",
    "        wav_path = os.path.join(self.audio_dir, row['file_name'] + \".wav\")\n",
    "\n",
    "        # Convert string label to numeric\n",
    "        label = self.label_map.get(row['label'], 0)\n",
    "\n",
    "        # Extract log-Mel spectrogram features\n",
    "        features = extract_features(\n",
    "            wav_path,\n",
    "            sample_rate=self.sample_rate,\n",
    "            duration=self.duration,\n",
    "            n_mels=self.n_mels\n",
    "        )\n",
    "\n",
    "        return features, torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Creating PyTorch Dataset objects for the English subset of the DECRO dataset ===\n",
    "\n",
    "# Initialize training dataset using the protocol file and corresponding audio directory\n",
    "train_dataset_english = DeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_train.txt\",       # Path to training protocol file\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_train\"        # Path to training audio (.wav) files\n",
    ")\n",
    "\n",
    "# Initialize development (validation) dataset\n",
    "development_dataset_english = DeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_dev.txt\",         # Path to dev protocol file\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_dev\"          # Path to dev audio files\n",
    ")\n",
    "\n",
    "# Initialize evaluation (test) dataset\n",
    "evaluation_dataset_english = DeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_eval.txt\",        # Path to eval protocol file\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_eval\"         # Path to eval audio files\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: torch.Size([1, 128, 94])\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# === Testing the Dataset with a single sample ===\n",
    "\n",
    "# Retrieve the first sample (index 0) from the training dataset\n",
    "sample_feature, sample_label = train_dataset_english[0]\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "# Expected shape: [1, 128, time]\n",
    "#   - 1: Channel dimension (used for CNNs, treated like grayscale image)\n",
    "#   - 128: Number of Mel frequency bins\n",
    "#   - time: Number of time steps (depends on duration and hop size used by librosa)\n",
    "print(\"Feature shape:\", sample_feature.shape)\n",
    "\n",
    "# Print the label for this sample\n",
    "# Expected: 0 for 'bonafide', 1 for 'spoof'\n",
    "print(\"Label:\", sample_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SiameseDeepfakeDataset: Prepares pairs of audio samples for Siamese Network training ===\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDeepfakeDataset(Dataset):\n",
    "    def __init__(self, protocol_file, audio_dir, sample_rate=16000, duration=3, n_mels=128):\n",
    "        # Initialize the base DeepfakeDataset for feature extraction\n",
    "        self.base_dataset = DeepfakeDataset(\n",
    "            protocol_file=protocol_file,\n",
    "            audio_dir=audio_dir,\n",
    "            sample_rate=sample_rate,\n",
    "            duration=duration,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "\n",
    "        # Precompute indices for each label to efficiently sample pairs\n",
    "        # Dictionary: {0: indices of bonafide samples, 1: indices of spoofed samples}\n",
    "        self.indices_by_label = {0: [], 1: []}\n",
    "        for idx in range(len(self.base_dataset)):\n",
    "            _, label = self.base_dataset[idx]\n",
    "            self.indices_by_label[label.item()].append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of samples in the base dataset\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the first sample (anchor)\n",
    "        feature1, label1 = self.base_dataset[idx]\n",
    "\n",
    "        # Decide randomly whether to create a pair from the same class or different class\n",
    "        same_class = np.random.rand() < 0.5\n",
    "\n",
    "        if same_class:\n",
    "            # Choose a second index with the same label (same class pair)\n",
    "            indices = self.indices_by_label[label1.item()]\n",
    "        else:\n",
    "            # Choose a second index from the opposite label (different class pair)\n",
    "            other_label = 1 - label1.item()\n",
    "            indices = self.indices_by_label[other_label]\n",
    "\n",
    "        # Safety check: if selected label set is empty (edge case), fallback to random sample\n",
    "        if len(indices) == 0:\n",
    "            idx2 = np.random.randint(0, len(self.base_dataset))\n",
    "        else:\n",
    "            # Randomly choose another sample index from the selected class\n",
    "            idx2 = np.random.choice(indices)\n",
    "\n",
    "        # Get the second sample (positive or negative depending on pair type)\n",
    "        feature2, label2 = self.base_dataset[idx2]\n",
    "\n",
    "        # Define the similarity target\n",
    "        #   0 → same class (positive pair)\n",
    "        #   1 → different class (negative pair)\n",
    "        target = 0.0 if label1.item() == label2.item() else 1.0\n",
    "\n",
    "        return feature1, feature2, torch.tensor([target], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Prepare the Siamese Dataset DataLoader ===\n",
    "\n",
    "# Create the dataset using the protocol and audio path for the English training set\n",
    "siamese_train_dataset = SiameseDeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_train.txt\",  # Protocol file with labels\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_train\"   # Corresponding .wav files directory\n",
    ")\n",
    "\n",
    "# Wrap the dataset in a DataLoader for batching and shuffling during training\n",
    "train_loader = DataLoader(\n",
    "    siamese_train_dataset,  # The dataset to sample from\n",
    "    batch_size=16,          # Number of pairs per batch\n",
    "    shuffle=True            # Shuffle the dataset each epoch for better generalization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                        # Core PyTorch library\n",
    "import torch.nn as nn               # Neural Network Modules\n",
    "import torch.nn.functional as F     # Functional Utilities(ReLU,pooling,distance,etc.)\n",
    "import torch.optim as optim         # Optimizers like Adam\n",
    "from torch.utils.data import DataLoader, Dataset    # Data Loading Utilities\n",
    "import numpy as np                  # For random choice and numerical operations\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameters & Settings\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 32             # Number of pairs per batch\n",
    "NUM_EPOCHS = 40             # Total training cycles\n",
    "LEARNING_RATE = 0.0005      # Learning Rate for Optimizer\n",
    "MARGIN = 2.0                # Margin for Contrastive Loss\n",
    "INPUT_CHANNELS = 1          # Spectograms are 1-channel\n",
    "INPUT_HEIGHT = 128          # Height of Spectograms (mel bands)\n",
    "INPUT_WIDTH = 128           # Width (time dimension , should be fixed by user)\n",
    "EMBEDDING_SIZE = 256        # Size of output vector from each image\n",
    "# ----------------------------\n",
    "# Residual Block Definition\n",
    "# ----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Second convolution\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # Shortcut connection to match dimensions if needed\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "# ----------------------------\n",
    "# Squeeze-and-Excitation Block\n",
    "# ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        # Squeeze: Global Average Pooling\n",
    "        y = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
    "        # Excitation: Fully connected layers with ReLU and Sigmoid activations\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "# ----------------------------\n",
    "# Advanced Siamese CNN Model\n",
    "# ----------------------------\n",
    "class RefinedSiameseCNN(nn.Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE):\n",
    "        super(RefinedSiameseCNN, self).__init__()\n",
    "\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(INPUT_CHANNELS, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(32, 64, num_blocks=2, stride=2)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.se = SEBlock(256)\n",
    "\n",
    "        # Delay FC layer creation — will define it after calculating shape\n",
    "        self.fc = None\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        out = self.initial_conv(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.se(out)\n",
    "        return out\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        out = self.forward_features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Create FC layer on first forward pass\n",
    "        if self.fc is None:\n",
    "            conv_output_size = out.size(1)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(conv_output_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, self.embedding_size)\n",
    "            )\n",
    "            # Move to same device\n",
    "            self.fc.to(out.device)\n",
    "\n",
    "        embedding = self.fc(out)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "# ----------------------------\n",
    "# Contrastive Loss Definition\n",
    "# ----------------------------\n",
    "import torch.nn as nn\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=MARGIN):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss\n",
    "# ----------------------------\n",
    "# Training Loop Function\n",
    "# ----------------------------\n",
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (img1, img2, label) in enumerate(dataloader):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "# ----------------------------\n",
    "# Example Main Function (Dataset part to be added later)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Use your real data loader here\n",
    "    dataloader = train_loader  # Make sure train_loader is defined above\n",
    "\n",
    "    # Instantiate the refined Siamese model, loss, and optimizer.\n",
    "    model = RefinedSiameseCNN(embedding_size=EMBEDDING_SIZE).to(device)\n",
    "    criterion = ContrastiveLoss(margin=MARGIN)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, dataloader, criterion, optimizer, device, num_epochs=NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.0131\n",
      "Epoch [2/20], Loss: 1.0050\n",
      "Epoch [3/20], Loss: 1.0023\n",
      "Epoch [4/20], Loss: 0.9950\n",
      "Epoch [5/20], Loss: 0.9922\n",
      "Epoch [6/20], Loss: 0.9875\n",
      "Epoch [7/20], Loss: 0.9877\n",
      "Epoch [8/20], Loss: 0.9902\n",
      "Epoch [9/20], Loss: 0.9874\n",
      "Epoch [10/20], Loss: 0.9865\n",
      "Epoch [11/20], Loss: 0.9846\n",
      "Epoch [12/20], Loss: 0.9886\n",
      "Epoch [13/20], Loss: 0.9856\n",
      "Epoch [14/20], Loss: 0.9864\n",
      "Epoch [15/20], Loss: 0.9851\n",
      "Epoch [16/20], Loss: 0.9827\n",
      "Epoch [17/20], Loss: 0.9818\n",
      "Epoch [18/20], Loss: 0.9832\n",
      "Epoch [19/20], Loss: 0.9849\n",
      "Epoch [20/20], Loss: 0.9836\n"
     ]
    }
   ],
   "source": [
    "# Entry point for the script.\n",
    "# This conditional ensures that the main training loop is only executed\n",
    "# when this script is run directly, and not when it is imported as a module.\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the development (validation) dataset and dataloader.\n",
    "# This will be used to monitor model performance during training (optional).\n",
    "siamese_dev_dataset = SiameseDeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_dev.txt\",     # Path to development label file\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_dev\"      # Path to corresponding WAV files\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    siamese_dev_dataset,\n",
    "    batch_size=16,    # Number of sample pairs per batch\n",
    "    shuffle=True      # Enable shuffling for generalization\n",
    ")\n",
    "\n",
    "# Instantiate the evaluation (test) dataset and dataloader.\n",
    "# This will be used for final model evaluation after training.\n",
    "siamese_eval_dataset = SiameseDeepfakeDataset(\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/en_eval.txt\",    # Path to evaluation label file\n",
    "    \"/home/kartik-dua/python_project/DECRO/inner/wav/en_eval\"     # Path to corresponding WAV files\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    siamese_eval_dataset,\n",
    "    batch_size=16,    # Number of sample pairs per batch\n",
    "    shuffle=True      # Shuffle may be used here for batch variability\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Device Setup and Model Initialization\n",
    "# ----------------------------\n",
    "\n",
    "# Set the computation device — GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the refined Siamese CNN model and move it to the selected device\n",
    "model = RefinedSiameseCNN(embedding_size=EMBEDDING_SIZE).to(device)\n",
    "\n",
    "# Define the contrastive loss function with a specified margin\n",
    "criterion = ContrastiveLoss(margin=MARGIN)\n",
    "\n",
    "# Use the Adam optimizer to update model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Validation Function\n",
    "# ----------------------------\n",
    "\n",
    "def validate(model, dataloader, criterion, device, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the Siamese model on a validation or evaluation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained Siamese network.\n",
    "        dataloader (DataLoader): DataLoader providing pairs of inputs and labels.\n",
    "        criterion (nn.Module): Contrastive loss function.\n",
    "        device (torch.device): Device on which to perform computation.\n",
    "        threshold (float): Distance threshold to classify a pair as \"different\" (1) or \"same\" (0).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout/batchnorm updates)\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking for validation\n",
    "        for x1, x2, labels in dataloader:\n",
    "            # Move inputs and labels to the target device\n",
    "            x1, x2, labels = x1.to(device), x2.to(device), labels.to(device)\n",
    "\n",
    "            # Flatten labels from shape [batch_size, 1] to [batch_size]\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            output1, output2 = model(x1, x2)\n",
    "\n",
    "            # Compute contrastive loss\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute Euclidean distances between embeddings\n",
    "            distances = F.pairwise_distance(output1, output2)\n",
    "\n",
    "            # Predict: same (0) if distance < threshold, different (1) if >= threshold\n",
    "            preds = (distances >= threshold).float()\n",
    "\n",
    "            # Compare predictions with actual labels\n",
    "            total_correct += (preds == labels).float().sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9843, Accuracy: 62.36%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.984286413192749, 0.6235539343408025)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Run Evaluation on the Evaluation Set\n",
    "# ----------------------------\n",
    "\n",
    "# Evaluate the trained Siamese model using the evaluation dataset.\n",
    "# A lower threshold (e.g., 0.8) can make the model more sensitive to subtle differences.\n",
    "validate(\n",
    "    model=model,\n",
    "    dataloader=eval_loader,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    threshold=0.8  # Distance threshold for classification: same vs different\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
